<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>离线部署私服方案</title>
      <link href="/2024/04/12/chi-xian-bu-shu-si-fu-fang-an/"/>
      <url>/2024/04/12/chi-xian-bu-shu-si-fu-fang-an/</url>
      
        <content type="html"><![CDATA[<p>搭建私服并配置好依赖，更换pip、apt等系统源，使其从私服中拉取依赖</p>]]></content>
      
      
      
        <tags>
            
            <tag> devops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>记一次大连理工实验室docker打包离线环境镜像的操作</title>
      <link href="/2024/04/09/ji-yi-ci-da-lian-li-gong-shi-yan-shi-docker-da-bao-chi-xian-huan-jing-jing-xiang-de-cao-zuo/"/>
      <url>/2024/04/09/ji-yi-ci-da-lian-li-gong-shi-yan-shi-docker-da-bao-chi-xian-huan-jing-jing-xiang-de-cao-zuo/</url>
      
        <content type="html"><![CDATA[<h2 id="需求分析"><a href="#需求分析" class="headerlink" title="需求分析"></a>需求分析</h2><p>实验室需要对一个大模型问答项目进行离线部署，项目主要使用了大语言模型，embedding模型和rerank模型。其中大语言模型使用fastchat进行部署，项目使用python语言开发，并且使用了关系型数据库和向量文档数据库Elasticsearch作为中间件。<br>之前在使用项目时，使用的大语言模型是百度的在线模型qianfan，而更换成本地大模型后，需要程序运行环境能够支持cuda和nvidia驱动，为了简化部署的流程，决定使用docker打包镜像的方式进行部署，需要让具备cuda的docker容器能够检测到宿主机的nvidia驱动，作为程序运行的环境。<br>由于程序代码需要不断改动，如果将代码COPY入dockerfile中进行统一打包，部署起来虽然会容易，但这样会带来一些问题：</p><ul><li>模型文件和代码文件量过大，导致镜像的大小和层数过大，构建镜像的速度会很慢，并且容器运行的效率也不高。</li><li>后期程序文件，配置文件可能会更改，也可能使用其他新的模型，如果统一打包，每次相关文件有改动就需要重新构建镜像，浪费很多时间和成本。</li><li>docker运行模型，要根据docker镜像的系统版本以及宿主机的nvidia驱动等等信息判断运行的程序能够穿透到cpu，使运行的效率能够达到最大，减少“虚拟机”容器带来的性能损耗，最大化运用宿主机的算力。</li></ul><h2 id="方案设计"><a href="#方案设计" class="headerlink" title="方案设计"></a>方案设计</h2><p>基于上述问题，在构建python主程序的dockerfile时，将程序代码和程序运行环境区分开来，dockerfile的目的是构建支持程序运行的环境，包括python和相关库的依赖，以及cuda计算框架和nvidia驱动调用。而对于中间件来说，使用docker-compose对其进行编排，设计好存储卷和docker网络，让各个容器之间能够互相调用，并且进行数据的持久化存储。</p><h3 id="主系统运行环境构建"><a href="#主系统运行环境构建" class="headerlink" title="主系统运行环境构建"></a>主系统运行环境构建</h3><p>使用nvidia&#x2F;cuda作为基础镜像，并且只copy requirements文件进行依赖加载，注意镜像附带的python环境，然后将系统的运行端口暴露出来，最后使用cmd将容器运行时的命令设计好，启动容器时会自动运行main.py文件启动项目。</p><h3 id="中间件环境构建以及容器编排"><a href="#中间件环境构建以及容器编排" class="headerlink" title="中间件环境构建以及容器编排"></a>中间件环境构建以及容器编排</h3><p>系统主要使用了elasticsearch中间件，为了支持elasticsearch的可视化管理，引入了kibana，并且设计了docker网络和存储卷。对于python程序代码和模型文件，使用了挂载宿主机目录的方式，将其挂载到容器内的工作目录中，这样后续直接在宿主机更改，然后重启容器即可。<br>将整个编排的逻辑编写成docker-compose文件，使用docker compose up -d 命令一建启动即可。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><h3 id="dockerfile设计"><a href="#dockerfile设计" class="headerlink" title="dockerfile设计"></a>dockerfile设计</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">FROM nvidia/cuda:12.1.1-cudnn8-devel-ubuntu22.04RUN <span class="token function">apt-get</span> update <span class="token punctuation">\</span><span class="token operator">&amp;&amp;</span> <span class="token assign-left variable">DEBIAN_FRONTEND</span><span class="token operator">=</span><span class="token string">"noninteractive"</span> <span class="token punctuation">\</span><span class="token function">apt-get</span> <span class="token function">install</span> <span class="token parameter variable">-y</span> <span class="token punctuation">\</span><span class="token function">git</span> <span class="token function">wget</span> <span class="token function">vim</span> python3-pip python3-venv libopencv-dev git-lfs <span class="token function">sudo</span> build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev libbz2-dev liblzma-dev <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span><span class="token function">apt-get</span> clean <span class="token operator">&amp;&amp;</span> <span class="token punctuation">\</span><span class="token function">rm</span> <span class="token parameter variable">-rf</span> /var/lib/apt/lists/*WORKDIR /appCOPY requirements.txt <span class="token builtin class-name">.</span>RUN python3 <span class="token parameter variable">-m</span> pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> pipRUN pip <span class="token function">install</span> <span class="token parameter variable">--upgrade</span> streamlitRUN pip <span class="token function">install</span> <span class="token parameter variable">-r</span> requirements.txtEXPOSE <span class="token number">8888</span>EXPOSE <span class="token number">8889</span>CMD <span class="token punctuation">[</span><span class="token string">"python3"</span>, <span class="token string">"main.py"</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="docker-compose设计"><a href="#docker-compose设计" class="headerlink" title="docker-compose设计"></a>docker-compose设计</h3><pre class="line-numbers language-none"><code class="language-none">services:  Report-Analysis-mysqldb:    image: mysql:8.0.33    networks:       - ra-net    volumes:      - ra-data:&#x2F;var&#x2F;lib&#x2F;mysql    ports:      - 3306:3306    environment:       - MYSQL_ROOT_PASSWORD&#x3D;ra      - MYSQL_DATABASE&#x3D;ra    command: [      &quot;--character-set-server&#x3D;utf8mb4&quot;,      &quot;--collation-server&#x3D;utf8mb4_general_ci&quot;,      &quot;--skip-character-set-client-handshake&quot;      ]  Report-Analysis-elasticsearchdb:      image: elasticsearch:8.5.3      container_name: Report-Analysis-elasticsearchdb      environment:        - discovery.type&#x3D;single-node        - ES_JAVA_OPTS&#x3D;-Xms1g -Xmx1g        - xpack.security.enabled&#x3D;false      volumes:        - ra-data:&#x2F;usr&#x2F;share&#x2F;elasticsearch&#x2F;data      ports:        - 9200:9200      networks:        - ra-net    Report-Analysis-kibana:    image: kibana:8.1.3    container_name: Report-Analysis-kibana    ports:      - 5601:5601    depends_on:      - Report-Analysis-elasticsearchdb    networks:      - ra-net    langchain-rag:    working_dir: &#x2F;app    image: langchain-rag:v0.0.1    container_name: langchain-rag    restart: always    volumes:      - .:&#x2F;app    ports:      - &quot;8888:8888&quot;      - &quot;8889:8889&quot;    depends_on:      - Report-Analysis-elasticsearchdb    environment:        - NVIDIA_VISIBLE_DEVICES&#x3D;all        - NVIDIA_DRIVER_CAPABILITIES&#x3D;all     deploy:        resources:          reservations:            devices:              - driver: nvidia                count: all                capabilities: [gpu]                options:                  - &quot;environment&#x3D;NVIDIA_VISIBLE_DEVICES&#x3D;all&quot;     networks:      - ra-netvolumes:  ra-data:networks:  ra-net:<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><h4 id="容器需要使用宿主机的nvidia驱动"><a href="#容器需要使用宿主机的nvidia驱动" class="headerlink" title="容器需要使用宿主机的nvidia驱动"></a>容器需要使用宿主机的nvidia驱动</h4><p>首次编写python运行的容器compose时（即langchain-rag），发现compose up启动后，容器运行的程序在需要使用模型文件进行运算时总是无法使用宿主机的nvidia驱动，而在宿主机使用nvidia-smi是有显卡信息的，说明宿主机驱动是正常的。而docker容器只包含了cuda，并不包含nvidia驱动，对于这种情况，有两种解决方案。</p><ul><li><p>在镜像内安装好nvidia驱动，即在dockerfile里增加一层，而docker支持直接穿透使用宿主机的驱动，这种方式一般不推荐使用</p></li><li><p>启动容器时通过 –gpus all 命令允许容器访问宿主机gpu。而由于使用了docker compose文件，具体操作如下所示<br>  您提供的 Dockerfile 基于 <code>nvidia/cuda:12.2.0-runtime-ubuntu20.04</code>，这个基础镜像只包含了 NVIDIA CUDA 工具包和运行时库，并不包含 NVIDIA 驱动。这是因为 <code>runtime</code> 版本的镜像旨在为已经在主机上安装了 NVIDIA 驱动的环境中运行 CUDA 应用程序提供所需的运行时组件。</p><p>  如果您需要在 Docker 容器中安装 NVIDIA 驱动，您应该使用 <code>nvidia/cuda:12.2.0-base-ubuntu20.04</code> 镜像作为基础，这个版本的镜像包含了 NVIDIA 驱动以及 CUDA 工具包和库。但是，请注意，在 Docker 容器中安装 NVIDIA 驱动通常是不推荐的，因为 Docker 容器应该尽可能保持轻量级和可移植。通常，驱动应该在宿主机上安装，而容器则依赖于宿主机的驱动。</p><p>  如果您确实需要在容器内安装驱动（尽管这并不常见），您可能需要编写额外的安装步骤到 Dockerfile 中，并可能需要使用 <code>nvidia-docker2</code> 或其他相关工具来确保驱动的正确安装和配置。</p><p>  如果您只是想运行一个基于 CUDA 的应用程序，并且已经在宿主机上安装了 NVIDIA 驱动，那么您当前的 Dockerfile 应该是足够的。您只需要确保在运行容器时，宿主机上的 NVIDIA 驱动与容器内的 CUDA 版本兼容即可。</p><p>  最后，请确保您的宿主机已经安装了 NVIDIA 驱动，并且已经正确配置了 CUDA 环境。在启动容器时，您可能还需要使用 <code>--gpus all</code> 参数来允许容器访问宿主机的 GPU。例如：</p>  <pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker</span> run <span class="token parameter variable">--gpus</span> all <span class="token parameter variable">-p</span> <span class="token number">8888</span>:8888 <span class="token parameter variable">-p</span> <span class="token number">8889</span>:8889 <span class="token parameter variable">-d</span> your_image_name<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>  这将会允许容器访问所有可用的 GPU，并将容器的 8888 和 8889 端口映射到宿主机的相应端口上。</p><p>  要将 <code>docker run</code> 命令转换为 <code>docker-compose.yml</code> 格式，您需要创建一个 <code>docker-compose.yml</code> 文件，并在其中定义服务及其配置。以下是一个示例，该示例将您的 <code>docker run</code> 命令转换为 <code>docker-compose.yml</code> 格式：</p></li></ul><pre class="line-numbers language-yaml" data-language="yaml"><code class="language-yaml"><span class="token key atrule">version</span><span class="token punctuation">:</span> <span class="token string">'3'</span><span class="token key atrule">services</span><span class="token punctuation">:</span>  <span class="token key atrule">your_service_name</span><span class="token punctuation">:</span>    <span class="token key atrule">image</span><span class="token punctuation">:</span> your_image_name    <span class="token key atrule">runtime</span><span class="token punctuation">:</span> nvidia  <span class="token comment"># 注意：这不是标准的 Compose 字段，可能需要根据您的配置调整</span>    <span class="token key atrule">environment</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> NVIDIA_VISIBLE_DEVICES=all      <span class="token punctuation">-</span> NVIDIA_DRIVER_CAPABILITIES=all    <span class="token key atrule">ports</span><span class="token punctuation">:</span>      <span class="token punctuation">-</span> <span class="token string">"8888:8888"</span>      <span class="token punctuation">-</span> <span class="token string">"8889:8889"</span>    <span class="token key atrule">deploy</span><span class="token punctuation">:</span>      <span class="token key atrule">resources</span><span class="token punctuation">:</span>        <span class="token key atrule">reservations</span><span class="token punctuation">:</span>          <span class="token key atrule">devices</span><span class="token punctuation">:</span>            <span class="token punctuation">-</span> <span class="token key atrule">driver</span><span class="token punctuation">:</span> nvidia              <span class="token key atrule">count</span><span class="token punctuation">:</span> all              <span class="token key atrule">capabilities</span><span class="token punctuation">:</span> <span class="token punctuation">[</span>gpu<span class="token punctuation">]</span>              <span class="token key atrule">options</span><span class="token punctuation">:</span>                <span class="token punctuation">-</span> <span class="token key atrule">environment</span><span class="token punctuation">:</span>                    <span class="token punctuation">-</span> NVIDIA_VISIBLE_DEVICES=all<span class="token comment"># 这段YAML配置是用于Docker Compose的一部分设置，它指定了容器部署时的资源需求，具体来说：</span><span class="token comment"># deploy: 这一字段下包含了有关部署容器的配置选项。</span><span class="token comment"># resources: 在deploy下，指定了容器运行所需要的资源。</span><span class="token comment"># reservations: 这里用于保留所需的系统资源。</span><span class="token comment"># devices: 指明容器可以访问的设备。</span><span class="token comment"># 其中 - driver: nvidia 表示使用NVIDIA的驱动，这用于支持Docker容器访问NVIDIA GPU。</span><span class="token comment"># count: all 告诉Docker需要使用所有可用的GPU设备。</span><span class="token comment"># capabilities: [gpu] 表示容器需要的设备能力是GPU。</span><span class="token comment"># options: 这里为设备指定了一些额外的选项。</span><span class="token comment"># - "environment=NVIDIA_VISIBLE_DEVICES=all" 实际上是设置一个环境变量NVIDIA_VISIBLE_DEVICES，它的值为all。这意味着Docker容器将能够看到和使用主机上所有可用的NVIDIA GPU设备。</span><span class="token comment"># 这种配置通常用于需要执行机器学习或其他高性能计算任务的应用，它们需要利用GPU加速计算能力。通过这样配置，Docker容器可以充分访问宿主机上的NVIDIA GPU资源。</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>在上面的 <code>docker-compose.yml</code> 文件中，我们定义了一个名为 <code>your_service_name</code> 的服务（您应该将其替换为您自己的服务名），该服务使用 <code>your_image_name</code> 镜像（您应该将其替换为您实际的镜像名）。<code>runtime: nvidia</code> 不是标准的 Docker Compose 配置项，如果您的 Compose 文件不支持这一项，请忽略或替换为其他必要的配置来确保 GPU 支持。</p><p><code>environment</code> 部分设置了两个环境变量，用于与 NVIDIA 驱动和运行时进行交互。</p><p><code>ports</code> 部分映射了宿主机的端口到容器的端口。</p><p><code>deploy</code> 部分的 <code>resources</code> 用来声明服务运行所需要的资源，包括 GPU 设备。这里我们指定使用所有的 GPU 设备，并设置了相关的能力（capabilities）和选项（options）。</p><p>保存文件后，在包含 <code>docker-compose.yml</code> 文件的目录中运行以下命令来启动服务：</p><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash"><span class="token function">docker-compose</span> up <span class="token parameter variable">-d</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>这将使用您在 <code>docker-compose.yml</code> 文件中定义的配置来启动容器，并将容器的 8888 和 8889 端口映射到宿主机的相应端口上。<code>-d</code> 参数表示在后台运行容器。</p><p>请确保您的 Docker 环境已经配置为支持 NVIDIA GPU，并且您的宿主机已经安装了 NVIDIA 驱动和 CUDA 工具包。此外，如果您使用的是 Docker Desktop for Windows 或 macOS，您可能需要安装额外的 NVIDIA 容器工具包来支持 GPU 加速。</p><h4 id="容器需要获取宿主机的显卡信息"><a href="#容器需要获取宿主机的显卡信息" class="headerlink" title="容器需要获取宿主机的显卡信息"></a>容器需要获取宿主机的显卡信息</h4><p>在程序运行时，为了加载大模型，会传入一些参数，例如</p><pre class="line-numbers language-none"><code class="language-none">args.gpus &#x3D; &quot;0&quot;  # GPU的编号,如果有多个GPU，可以设置为&quot;0,1,2,3&quot;      args.max_gpu_memory &#x3D; &quot;22GiB&quot;      args.num_gpus &#x3D; 1  # model worker的切分是model并行，这里填写显卡的数量      args.load_8bit &#x3D; False      args.cpu_offloading &#x3D; None      args.gptq_ckpt &#x3D; None      args.gptq_wbits &#x3D; 16      args.gptq_groupsize &#x3D; -1      args.gptq_act_order &#x3D; False      args.awq_ckpt &#x3D; None      args.awq_wbits &#x3D; 16      args.awq_groupsize &#x3D; -1      args.model_names &#x3D; [&quot;&quot;]      args.conv_template &#x3D; None      args.limit_worker_concurrency &#x3D; 5      args.stream_interval &#x3D; 2      args.no_register &#x3D; False      args.embed_in_truncate &#x3D; False      for k, v in kwargs.items():          setattr(args, k, v)      if args.gpus:          if args.num_gpus is None:              args.num_gpus &#x3D; len(args.gpus.split(&#39;,&#39;))          if len(args.gpus.split(&quot;,&quot;)) &lt; args.num_gpus:              raise ValueError(                  f&quot;Larger --num-gpus (&#123;args.num_gpus&#125;) than --gpus &#123;args.gpus&#125;!&quot;              )          os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] &#x3D; args.gpus<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><p>而在实际运行时，发现<code>os.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] = args.gpus</code>这条语句无法达到效果，为了解决这个问题，将命令写死，放在程序入口的开头。</p><pre class="line-numbers language-none"><code class="language-none">import osos.environ[&quot;CUDA_VISIBLE_DEVICES&quot;] &#x3D; &quot;1&quot;<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span></span></code></pre><p>并且要注意，这条语句要放在import torch之前。</p><h3 id="最终效果"><a href="#最终效果" class="headerlink" title="最终效果"></a>最终效果</h3><p><img src="https://trueno-imgbed.oss-cn-hangzhou.aliyuncs.com/typoraImg/image-20240410001301161.png" alt="image-20240410001301161"><br><img src="https://trueno-imgbed.oss-cn-hangzhou.aliyuncs.com/typoraImg/image-20240410001635783.png" alt="image-20240410001635783"></p><h2 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h2><ul><li>dockerfile编写，挂载工作目录使运行环境和源文件分离</li><li>docker-compose设计deploy，通过参数和环境变量使容器能使用宿主机的显卡能力</li><li>docker logs &lt;容器id&#x2F;容器名&gt; 查看容器终端的输出</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> devops </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2024/02/11/hello-world/"/>
      <url>/2024/02/11/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo new <span class="token string">"My New Post"</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/writing.html">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo server<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/server.html">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo generate<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/generating.html">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><pre class="line-numbers language-bash" data-language="bash"><code class="language-bash">$ hexo deploy<span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><p>More info: <a href="https://hexo.io/docs/one-command-deployment.html">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
